{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a36339a-4ed7-4fab-9e3b-c12d79c34103",
   "metadata": {},
   "source": [
    "# NMF and Movie Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bd3160f-a573-43f4-8873-12fc5357bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import NMF\n",
    "from helpers.metrics import *\n",
    "\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from scipy.spatial.distance import jaccard, cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5106fef-2195-4592-8d0f-32d1eb54f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# please, no warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "os.environ['PYTHONWARNINGS']='ignore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39595952-e69d-4020-b7d5-a98197429737",
   "metadata": {},
   "outputs": [],
   "source": [
    "MV_users = pd.read_csv('./data/movies/users.csv')\n",
    "MV_movies = pd.read_csv('./data/movies/movies.csv')\n",
    "train = pd.read_csv('./data/movies/train.csv')\n",
    "test = pd.read_csv('./data/movies/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1bd07b-9c1b-4f01-87c9-989437972e00",
   "metadata": {},
   "source": [
    "## Section 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec00ca71-7e63-4259-a0b3-456a61bcb0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From CSCA 5632 Week 3 Lab \n",
    "def jaccard_sparse(mat):\n",
    "    intersection = mat.dot(mat.T)\n",
    "    \n",
    "    row_sums = mat.sum(axis=1).A1\n",
    "\n",
    "    union = row_sums[:, None] + row_sums[None, :] - intersection.toarray()\n",
    "    \n",
    "    jaccard_sim = intersection.toarray() / union\n",
    "    jaccard_sim[union == 0] = 0.0\n",
    "\n",
    "    return jaccard_sim\n",
    "\n",
    "# (⊙_☉) (°~°) (•_•?)\n",
    "def jaccard_sparse_multi(original, multi_input):\n",
    "    intersection = (multi_input[0].dot(multi_input[0].T)).toarray()\n",
    "    for mat in multi_input[1:]:\n",
    "        to_add = (mat.dot(mat.T)).toarray()\n",
    "        intersection += to_add\n",
    "        \n",
    "    row_sums = original.getnnz(axis=1)\n",
    "    augmented = np.tile(row_sums, (row_sums.shape[0], 1))\n",
    "    union = augmented + augmented.T - intersection\n",
    "    \n",
    "    jaccard_sim = intersection / union\n",
    "\n",
    "    return jaccard_sim\n",
    "\n",
    "def cos_sparse(mat):\n",
    "    dot_product = mat.dot(mat.T).toarray()\n",
    "    norms = np.sqrt(mat.multiply(mat).sum(axis=1))\n",
    "    denom = norms.dot(norms.T)\n",
    "    cos_sim = dot_product / denom\n",
    "    cos_sim = np.nan_to_num(cos_sim, nan=0.0)\n",
    "    cos_sim = 0.5 * cos_sim + 0.5\n",
    "    np.fill_diagonal(cos_sim, 1.0)\n",
    "    return np.array(cos_sim)\n",
    "\n",
    "class RecSys():\n",
    "    def __init__(self,data):\n",
    "        self.data=data\n",
    "        self.allusers = list(self.data.users['uID'])\n",
    "        self.allmovies = list(self.data.movies['mID'])\n",
    "        self.genres = list(self.data.movies.columns.drop(['mID', 'title', 'year']))\n",
    "        self.mid2idx = dict(zip(self.data.movies.mID,list(range(len(self.data.movies)))))\n",
    "        self.uid2idx = dict(zip(self.data.users.uID,list(range(len(self.data.users)))))\n",
    "        self.Mr=self.rating_matrix()\n",
    "        self.Mm=None \n",
    "        self.sim=np.zeros((len(self.allmovies),len(self.allmovies)))\n",
    "        \n",
    "    def rating_matrix(self):\n",
    "        \"\"\"\n",
    "        Convert the rating matrix to numpy array of shape (#allusers,#allmovies)\n",
    "        \"\"\"\n",
    "        ind_movie = [self.mid2idx[x] for x in self.data.train.mID] \n",
    "        ind_user = [self.uid2idx[x] for x in self.data.train.uID]\n",
    "        rating_train = list(self.data.train.rating)\n",
    "        \n",
    "        return np.array(coo_matrix((rating_train, (ind_user, ind_movie)), shape=(len(self.allusers), len(self.allmovies))).toarray())\n",
    "\n",
    "\n",
    "    def predict_everything_to_3(self):\n",
    "        \"\"\"\n",
    "        Predict everything to 3 for the test data\n",
    "        \"\"\"\n",
    "        # Generate an array with 3s against all entries in test dataset\n",
    "        # your code here\n",
    "        toReturn  = np.copy(self.data.test)\n",
    "        return np.take((3,3,3,3,3,3), np.array(self.data.test.rating))\n",
    "        \n",
    "    def predict_to_user_average(self):\n",
    "        \"\"\"\n",
    "        Predict to average rating for the user.\n",
    "        Returns numpy array of shape (#users,)\n",
    "        \"\"\"\n",
    "        # Generate an array as follows:\n",
    "        # 1. Calculate all avg user rating as sum of ratings of user across all movies/number of movies whose rating > 0\n",
    "        # 2. Return the average rating of users in test data\n",
    "        # your code here\n",
    "#         combined = pd.concat([self.data.train, self.data.test])\n",
    "        combined = self.data.train\n",
    "        lookup = {}\n",
    "        for uID, mID, rating in zip(combined.uID, combined.mID, combined.rating):\n",
    "            if rating == 0:\n",
    "                continue\n",
    "            if uID not in lookup:\n",
    "                lookup[uID] = [rating, 1]\n",
    "            else:\n",
    "                lookup[uID][0] += rating\n",
    "                lookup[uID][1] += 1\n",
    "        \n",
    "        toReturn = []\n",
    "        for uID in self.data.test.uID:\n",
    "            if uID not in lookup:\n",
    "                toReturn.append(3)\n",
    "                continue\n",
    "            toReturn.append(lookup[uID][0]/lookup[uID][1])\n",
    "        \n",
    "        return np.array(toReturn)\n",
    "    \n",
    "    def predict_from_sim(self,uids,mids):\n",
    "        \"\"\"\n",
    "        Predict a user rating on a movie given userID and movieID\n",
    "        \"\"\"\n",
    "        # Predict user rating as follows:\n",
    "        # 1. Get entry of user id in rating matrix\n",
    "        # 2. Get entry of movie id in sim matrix\n",
    "        # 3. Employ 1 and 2 to predict user rating of the movie\n",
    "        # your code here\n",
    "        is_single = np.isscalar(uids) and np.isscalar(mids)\n",
    "        \n",
    "        if is_single:\n",
    "            uids = np.array([uids])\n",
    "            mids = np.array([mids])\n",
    "            \n",
    "        predicted_ratings = np.zeros(len(uids), dtype=np.float32)\n",
    "        \n",
    "        for idx, (uid,mid) in enumerate(zip(uids, mids)):\n",
    "            userEntry = self.Mr[self.uid2idx[uid]]\n",
    "            movieEntry = self.sim[self.mid2idx[mid]]\n",
    "\n",
    "            rated_mask = userEntry > 0\n",
    "            \n",
    "            total_weight = np.sum(movieEntry[rated_mask])\n",
    "            cumulative_rating = np.dot(movieEntry[rated_mask], userEntry[rated_mask])\n",
    "\n",
    "            predicted_ratings[idx] = cumulative_rating / total_weight\n",
    "                \n",
    "        return predicted_ratings[0] if is_single else predicted_ratings\n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Predict ratings in the test data. Returns predicted rating in a numpy array of size (# of rows in testdata,)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return self.predict_from_sim(self.data.test.uID, self.data.test.mID)\n",
    "    \n",
    "    def rmse(self,yp):\n",
    "        yp[np.isnan(yp)]=3 #In case there is nan values in prediction, it will impute to 3.\n",
    "        yt=np.array(self.data.test.rating)\n",
    "        return np.sqrt(((yt-yp)**2).mean())\n",
    "\n",
    "    \n",
    "class ContentBased(RecSys):\n",
    "    def __init__(self,data):\n",
    "        super().__init__(data)\n",
    "        self.data=data\n",
    "        self.Mm = self.calc_movie_feature_matrix()  \n",
    "        \n",
    "    def calc_movie_feature_matrix(self):\n",
    "        \"\"\"\n",
    "        Create movie feature matrix in a numpy array of shape (#allmovies, #genres) \n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        \n",
    "        return csr_matrix(np.array(self.data.movies[self.data.movies.columns.drop(['mID', 'title', 'year'])]))\n",
    "    \n",
    "    def calc_item_item_similarity(self):\n",
    "        \"\"\"\n",
    "        Create item-item similarity using Jaccard similarity\n",
    "        \"\"\"\n",
    "        # Update the sim matrix by calculating item-item similarity using Jaccard similarity\n",
    "        # Jaccard Similarity: J(A, B) = |A∩B| / |A∪B| \n",
    "        # your code here\n",
    "\n",
    "        self.sim = jaccard_sparse(self.Mm)\n",
    "                \n",
    "class Collaborative(RecSys):    \n",
    "    def __init__(self,data):\n",
    "        super().__init__(data)\n",
    "        \n",
    "    def calc_item_item_similarity(self, simfunction, *X):  \n",
    "        \"\"\"\n",
    "        Create item-item similarity using similarity function. \n",
    "        X is an optional transformed matrix of Mr\n",
    "        \"\"\"    \n",
    "        # General function that calculates item-item similarity based on the sim function and data inputed\n",
    "        if len(X)==0:\n",
    "            self.sim = simfunction()            \n",
    "        else:\n",
    "            self.sim = simfunction(X[0]) # *X passes in a tuple format of (X,), to X[0] will be the actual transformed matrix\n",
    "            \n",
    "    def cossim(self):    \n",
    "        \"\"\"\n",
    "        Calculates item-item similarity for all pairs of items using cosine similarity (values from 0 to 1) on utility matrix\n",
    "        Returns a cosine similarity matrix of size (#all movies, #all movies)\n",
    "        \"\"\"\n",
    "        # Return a sim matrix by calculating item-item similarity for all pairs of items using Jaccard similarity\n",
    "        # Cosine Similarity: C(A, B) = (A.B) / (||A||.||B||) \n",
    "        # your code here\n",
    "        \n",
    "        num_users = self.Mr.shape[0]\n",
    "        num_items = self.Mr.shape[1]\n",
    "        \n",
    "        entries = np.zeros((num_users, num_items), dtype=np.float32)\n",
    "        \n",
    "        for i, userEntry in enumerate(self.Mr):\n",
    "            mask = userEntry > 0\n",
    "            cumulative_rating = userEntry[mask].sum()\n",
    "            count = mask.sum()\n",
    "            \n",
    "            avg = cumulative_rating / count if count > 0 else 0\n",
    "            \n",
    "            newEntry = userEntry.astype(float)\n",
    "            newEntry[userEntry == 0] = avg\n",
    "            newEntry -= avg\n",
    "            \n",
    "            entries[i] = newEntry\n",
    "            \n",
    "        item_item_mat = csr_matrix(entries.T)\n",
    "        return cos_sparse(item_item_mat)\n",
    "    \n",
    "    def jacsim(self,Xr):\n",
    "        \"\"\"\n",
    "        Calculates item-item similarity for all pairs of items using jaccard similarity (values from 0 to 1)\n",
    "        Xr is the transformed rating matrix.\n",
    "        \"\"\"    \n",
    "        # Return a sim matrix by calculating item-item similarity for all pairs of items using Jaccard similarity\n",
    "        # Jaccard Similarity: J(A, B) = |A∩B| / |A∪B| \n",
    "        # your code here\n",
    "        \n",
    "        multi_input = []\n",
    "        for i in range(1, 6):\n",
    "            toAdd = Xr.T.astype(int)\n",
    "            toAdd[toAdd!=i] = 0\n",
    "            toAdd = csr_matrix((toAdd > 0).astype(int))\n",
    "            multi_input.append(toAdd)\n",
    "        \n",
    "        original = Xr.T.astype(int)\n",
    "        original = (original > 0).astype(int)\n",
    "        original = csr_matrix(original)\n",
    "        \n",
    "        return jaccard_sparse_multi(original, multi_input)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e356ef4-7c5d-4c90-b742-7f5e9c48cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9516534263875664\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "Data = namedtuple('Data', ['users','movies','train','test'])\n",
    "data = Data(MV_users, MV_movies, train, test)\n",
    "\n",
    "# sample prediction using jaccard similarity\n",
    "cf = Collaborative(data)\n",
    "Xr = cf.Mr.astype(int)\n",
    "cf.calc_item_item_similarity(cf.jacsim,Xr)\n",
    "\n",
    "yp = cf.predict()\n",
    "rmse = cf.rmse(yp)\n",
    "print(rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd495f7-0190-404f-b656-f1b383496b73",
   "metadata": {},
   "source": [
    "#### Strategy:\n",
    "\n",
    "For incorporating we will decompose the transpose of the utility matrix to create |Movies| x |Movies| matrix resembling the similarity matrix. This will in turn be used to make the prediction and subsequent RMSE measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f60beb5d-1099-49e4-b09f-9d5032a78a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=cf.Mr.T.shape[0],\n",
    "          init='nndsvda', \n",
    "          solver = 'mu',\n",
    "          beta_loss = 'kullback-leibler',\n",
    "          random_state = 0)\n",
    "\n",
    "nmf_train = nmf.fit_transform(cf.Mr.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "4c2e7c75-bd6d-4aca-b126-309661f6c435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.60571864e-014 1.76550360e-016 2.42709097e-013 ... 5.05518086e-015\n",
      "  5.05518086e-015 5.05518086e-015]\n",
      " [9.28009722e-033 1.28603181e-060 1.86054461e-046 ... 1.48724292e-028\n",
      "  1.48724292e-028 1.48724292e-028]\n",
      " [2.61173896e-040 1.23512673e-049 1.23051415e-042 ... 4.49394323e-027\n",
      "  4.49394323e-027 4.49394323e-027]\n",
      " ...\n",
      " [1.44632292e-166 6.97685816e-060 2.94821973e-049 ... 7.60257988e-114\n",
      "  7.60257988e-114 7.60257988e-114]\n",
      " [0.00000000e+000 2.34038144e-057 0.00000000e+000 ... 6.39304729e-158\n",
      "  6.39304729e-158 6.39304729e-158]\n",
      " [3.00826454e-042 2.43019398e-059 5.31564600e-030 ... 2.64268168e-049\n",
      "  2.64268168e-049 2.64268168e-049]]\n"
     ]
    }
   ],
   "source": [
    "sim = nmf.transform(cf.Mr.T)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3c0a67b6-dcca-4b2b-be32-24d54ea17353",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.sim = sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "fc8aab70-d7c0-4983-a1bc-b3842d8695fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "yp_part1 = cf.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "630d679c-3d25-493f-a7ec-7cb0d6533e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2234350967025864\n"
     ]
    }
   ],
   "source": [
    "rmse = cf.rmse(yp_part1)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef055d40-f494-4594-a0d9-9343ff98550d",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "We achieve a root mean squared error of 1.223. Previously in Week 3 assignment, we achieved a baseline RMSE of 1.259 for predicting all values to 3 and 1.035 for predicting values to user average. Using NMF as laid out in our strategy achieves an RMSE that is not much better than the baseline methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1903d3-ea22-417a-b3dd-50e10a859932",
   "metadata": {},
   "source": [
    "## Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f19c96-07d9-4f7b-86a3-36b5c4852905",
   "metadata": {},
   "source": [
    "As previously mentioned, the naive NMF strategy does not perform better than baseline methods explored in Week 3. This is because the columns in our new \"similarity matrix\" does not correspond to the specific movies as it did in the baseline methods. Instead they are latent features, the interpretability of which does not align with the columns of the similarity matrix manually calculated with similarity metrics such as Jaccard or Cosine distances. Rather than attempting to recreate a similarity matrix, we can instead try to directly predict the rating given a user ID and movie ID. The way to achieve this is to decompose the utility matrix into the W and H matrices using NMF. Then we can take the vectorized form of latent factors for the user id and similarly for the movie id and take the dot product. The prediction can then be scaled using robust scaler and/or min-max scaler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6c6b04b-619b-409b-b48d-48ce60236710",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf2 = NMF(n_components=15, \n",
    "           init='nndsvda', \n",
    "           solver = 'mu',\n",
    "           beta_loss = 'kullback-leibler',\n",
    "           random_state=0)\n",
    "\n",
    "# Decompose R into W and H\n",
    "W = nmf2.fit_transform(cf.Mr)\n",
    "H = nmf2.components_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6daae784-1bcd-4494-b96b-85d27cdcdf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, movie_id, W, H):\n",
    "    return np.dot(W[user_id, :], H[:, movie_id])\n",
    "\n",
    "yp = np.array([predict_rating(cf.uid2idx[uID], cf.mid2idx[mID], W, H) for uID, mID in zip(cf.data.test.uID, cf.data.test.mID)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5382b5f7-f86b-4ff9-866e-0e8e329f774f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.37768849 1.19816637 1.05769753 ... 1.10173326 1.01064584 1.05809781] 300063\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "\n",
    "# robust_scaler = RobustScaler()\n",
    "# yp_scaled = robust_scaler.fit_transform(yp.reshape(yp.shape[0], 1))\n",
    "\n",
    "minmax_scaler = MinMaxScaler(feature_range=(1, 5))\n",
    "yp_scaled = minmax_scaler.fit_transform(yp.reshape(yp.shape[0], 1))\n",
    "yp_scaled = yp_scaled.reshape(yp_scaled.shape[0])\n",
    "print(yp_scaled, len(yp_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2506a21a-8533-4077-8548-9efe16463c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.5837748338793403)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf.rmse(yp_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38106f18-68b1-4b50-8d91-b9c3c04d8a1b",
   "metadata": {},
   "source": [
    "The second strategy seems to perform worse than the first. We can try incorporating user averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15a4f213-6bd7-477b-b905-c4173f010061",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_averages = {}\n",
    "for uID in cf.allusers:\n",
    "    uIDX = cf.uid2idx[uID]\n",
    "    ratings_sum = sum(cf.Mr[uIDX])\n",
    "    user_averages[uIDX] = ratings_sum / np.count_nonzero(cf.Mr[uIDX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bbaaf5ba-276a-4393-b59a-a1786069bc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.34408001 1.16018886 1.05097098 ... 1.10914795 1.00981637 1.04049062] 300063\n"
     ]
    }
   ],
   "source": [
    "yp = np.array([predict_rating(cf.uid2idx[uID], cf.mid2idx[mID], W, H) * user_averages[cf.uid2idx[uID]] \n",
    "               for uID, mID in zip(cf.data.test.uID, cf.data.test.mID)])\n",
    "# yp = np.array([user_averages[cf.uid2idx[uID]] \n",
    "#                for uID, mID in zip(cf.data.test.uID, cf.data.test.mID)])\n",
    "\n",
    "minmax_scaler = MinMaxScaler(feature_range=(1, 5))\n",
    "yp = minmax_scaler.fit_transform(yp.reshape(yp.shape[0], 1))\n",
    "yp = yp.reshape(yp.shape[0])\n",
    "print(yp, len(yp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b864b78e-041e-418a-9817-48736e1faac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.5794950056062227)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf.rmse(yp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7496ec1-8ebd-488b-8c4a-ad4a514cbbf5",
   "metadata": {},
   "source": [
    "As we can see, there is not much improvement. Although the strategy as described in this section seemed promising as it had greater interpretability than the strategy in the first section, we will need to additional investgation to create a more viable way to use NMF for prediction task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
